{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandesh-kun/Vton/blob/main/TfinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accessing Google Drive**"
      ],
      "metadata": {
        "id": "_C5iPZEqHc32"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GeZ1O2aHIdl",
        "outputId": "3de4024b-29b9-40a0-fdaa-b92714f173f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo cp -v -r \"/content/drive/MyDrive/ladi-vton/\" \"/content/drive/MyDrive/Final_Project\""
      ],
      "metadata": {
        "id": "9al6rN-AHrnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Directory**"
      ],
      "metadata": {
        "id": "CYXet99nIW9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Final_Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF7EaLbEIVto",
        "outputId": "632e27f3-84d3-45ef-8d4f-f8aaf5f90b6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 torchvision==0.15.2 opencv-python==4.7.0.72 diffusers==0.14.0 transformers==4.27.3 accelerate==0.18.0 clean-fid==0.1.35 torchmetrics[image]==0.11.4 wandb==0.14.0 matplotlib==3.7.1 tqdm xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuOLKR0RJDEh",
        "outputId": "993285c9-c48c-4e41-c153-ed5423594c2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision==0.15.2 in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Collecting opencv-python==4.7.0.72\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diffusers==0.14.0\n",
            "  Downloading diffusers-0.14.0-py3-none-any.whl (737 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.27.3\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.18.0\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting clean-fid==0.1.35\n",
            "  Downloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\n",
            "Collecting torchmetrics[image]==0.11.4\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.14.0\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.21-cp310-cp310-manylinux2014_x86_64.whl (167.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.0/167.0 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.15.2) (9.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0) (6.8.0)\n",
            "Collecting huggingface-hub>=0.10.0 (from diffusers==0.14.0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.14.0) (2023.6.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.3) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.3) (6.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.3)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.18.0) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from clean-fid==0.1.35) (1.10.1)\n",
            "Collecting lpips<=0.1.4 (from torchmetrics[image]==0.11.4)\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-fidelity<=0.3.0 (from torchmetrics[image]==0.11.4)\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.14.0) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.14.0)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb==0.14.0)\n",
            "  Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb==0.14.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb==0.14.0)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb==0.14.0)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.14.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.14.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.14.0) (3.20.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (16.0.6)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.14.0) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.14.0)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.14.0) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.15.2) (2023.7.22)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.14.0) (3.16.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.14.0)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=0bf1ee75dd103a41faa900b747754b20fd50f0e4d216c710d5869f73421311b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, smmap, setproctitle, sentry-sdk, opencv-python, docker-pycreds, huggingface-hub, gitdb, transformers, GitPython, diffusers, wandb, torchmetrics, torch-fidelity, lpips, xformers, clean-fid, accelerate\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "Successfully installed GitPython-3.1.32 accelerate-0.18.0 clean-fid-0.1.35 diffusers-0.14.0 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.16.4 lpips-0.1.4 opencv-python-4.7.0.72 pathtools-0.1.2 sentry-sdk-1.30.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 torch-fidelity-0.3.0 torchmetrics-0.11.4 transformers-4.27.3 wandb-0.14.0 xformers-0.0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking Cuda**"
      ],
      "metadata": {
        "id": "u0d-vdBQJkH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHnP-A7aJpVz",
        "outputId": "0858989b-9910-4784-cf3a-6bf035a7863d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearing files from input and results folders"
      ],
      "metadata": {
        "id": "nWv4OpVHJ_jJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "files = glob.glob('input/*/*/*.*')\n",
        "for f in files:\n",
        "  os.remove(f)\n",
        "\n",
        "files = glob.glob('results/*/*/*.*')\n",
        "for f in files:\n",
        "  os.remove(f)"
      ],
      "metadata": {
        "id": "bRe3jy4AKKYO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resize Image 384x512"
      ],
      "metadata": {
        "id": "Ytzq4zZWKRoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_with_pad(im, target_width, target_height):\n",
        "    '''\n",
        "    Resize PIL image keeping ratio and using white background.\n",
        "    '''\n",
        "    target_ratio = target_height / target_width\n",
        "    im_ratio = im.height / im.width\n",
        "    if target_ratio > im_ratio:\n",
        "        # It must be fixed by width\n",
        "        resize_width = target_width\n",
        "        resize_height = round(resize_width * im_ratio)\n",
        "    else:\n",
        "        # Fixed by height\n",
        "        resize_height = target_height\n",
        "        resize_width = round(resize_height / im_ratio)\n",
        "\n",
        "    image_resize = im.resize((resize_width, resize_height), Image.ANTIALIAS)\n",
        "    background = Image.new('RGBA', (target_width, target_height), (255, 255, 255, 255))\n",
        "    offset = (round((target_width - resize_width) / 2), round((target_height - resize_height) / 2))\n",
        "    background.paste(image_resize, offset)\n",
        "    return background.convert('RGB')"
      ],
      "metadata": {
        "id": "pPl6SEIyKit0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process Resize images Input and clothes images parallely"
      ],
      "metadata": {
        "id": "Vxge864dL6M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add pairs\n",
        "def write_row(file_, *columns):\n",
        "    print(*columns, sep='\\t', end='\\n', file=file_)\n",
        "\n",
        "upper = open('input/upper_body/test_pairs_unpaired.txt', 'w')\n",
        "lower = open('input/lower_body/test_pairs_unpaired.txt', 'w')\n",
        "dresses = open('input/dresses/test_pairs_unpaired.txt', 'w')\n",
        "all = open('input/test_pairs_paired.txt', 'w')\n",
        "\n",
        "with open('images/test_pairs.txt', \"r\") as file:\n",
        "    data = file.readlines()\n",
        "    for line in data:\n",
        "        word = line.split()\n",
        "        if len(word) >= 3:  # Check if the line has at least 3 columns\n",
        "            org_path = 'images/humans/' + word[0]\n",
        "            if(word[2] == '0'):\n",
        "                write_row(upper, '0'+word[0], word[1])\n",
        "                write_row(all, '0'+word[0], word[1], word[2])\n",
        "                res_path = 'input/upper_body/images/0' + word[0]\n",
        "            elif(word[2] == '1'):\n",
        "                write_row(lower, '1'+word[0], word[1])\n",
        "                write_row(all, '1'+word[0], word[1], word[2])\n",
        "                res_path = 'input/lower_body/images/1' + word[0]\n",
        "            elif(word[2] == '2'):\n",
        "                write_row(dresses, '2'+word[0], word[1])\n",
        "                write_row(all, '2'+word[0], word[1], word[2])\n",
        "                res_path = 'input/dresses/images/2' + word[0]\n",
        "            image = Image.open(org_path)\n",
        "            new = resize_with_pad(image, 384, 512)\n",
        "            new.save(res_path)\n",
        "\n",
        "upper.close()\n",
        "lower.close()\n",
        "dresses.close()\n",
        "all.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1el2hA8VrzTw",
        "outputId": "1faa2dad-611a-4cb8-9df2-e90b428cc5e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-e04c8aed8ec9>:16: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image_resize = im.resize((resize_width, resize_height), Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhwaZkl4MMCd",
        "outputId": "0f1a8d5a-5398-4556-bcbe-baca71e65bb0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project/preprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cloning Pytorch Openpose into Prprocess folder"
      ],
      "metadata": {
        "id": "Np-o8QEkMZBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Hzzone/pytorch-openpose.git\n",
        "%cd pytorch-openpose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7xgdsKRMPbi",
        "outputId": "609bbd02-2d96-4585-e6a7-627b33f13f73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pytorch-openpose' already exists and is not an empty directory.\n",
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project/preprocess/pytorch-openpose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image is Processed in 3 classes, Upper, Lower, Dresses for pose estimation"
      ],
      "metadata": {
        "id": "Pp0h6wdIMmcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import numpy as np\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "from src import model\n",
        "from src import util\n",
        "from src.body import Body\n",
        "from src.hand import Hand\n",
        "\n",
        "body_estimation = Body('model/body_pose_model.pth')\n",
        "for s in ['upper_body','lower_body','dresses']:\n",
        "  input_path = '/content/drive/MyDrive/Final_Project/input/' + s + '/images/'\n",
        "  output_path = '/content/drive/MyDrive/Final_Project/input/'+ s + '/skeletons/'\n",
        "  keypoint_path = '/content/drive/MyDrive/Final_Project/input/'+ s + '/keypoints/'\n",
        "\n",
        "\n",
        "  for images in glob.glob('*',root_dir = input_path):\n",
        "      oriImg = cv2.imread(input_path+images)  # B,G,R order\n",
        "      candidate, subset = body_estimation(oriImg)\n",
        "      canvas = util.draw_bodypose(np.zeros_like(oriImg), candidate, subset)\n",
        "      arr = candidate.tolist()\n",
        "      vals = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0]\n",
        "      for i in range(0,18):\n",
        "        if len(arr)==i or arr[i][3] != vals[i]:\n",
        "          arr.insert(i,[-1,-1,-1,vals[i]])\n",
        "\n",
        "      keypoints = {'keypoints':arr[:18]}\n",
        "      cv2.imwrite(output_path + images.replace('_0','_5'),canvas)\n",
        "      with open(keypoint_path+ os.path.splitext(images)[0].replace('_0','_2') +\".json\" , 'w') as fin:\n",
        "        fin.write(json.dumps(keypoints))"
      ],
      "metadata": {
        "id": "TYpzGDjtNJK-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL07G-VEOA0Q",
        "outputId": "677670c6-25ac-4a59-8ae2-b529dffcb166"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYcxO9afOP6O",
        "outputId": "6e24b08c-4f0f-4690-d3a2-57899d151265"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project/preprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK5dsbwVOTY4",
        "outputId": "b86237dd-2151-4078-89f5-c3115bc2a775"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/146.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m122.9/146.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing images for Parsing"
      ],
      "metadata": {
        "id": "QdQykvEkPgug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PeikeLi/Self-Correction-Human-Parsing\n",
        "%cd Self-Correction-Human-Parsing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBSxIOCKOWPV",
        "outputId": "67985550-5a68-4d11-f0dc-b9decfdcf842"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Self-Correction-Human-Parsing' already exists and is not an empty directory.\n",
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project/preprocess/Self-Correction-Human-Parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_extractor.py --dataset 'atr' --model-restore '/content/drive/MyDrive/Final_Project/preprocess/Self-Correction-Human-Parsing/checkpoints/final.pth' --input-dir '/content/drive/MyDrive/Final_Project/input/upper_body/images/' --output-dir '/content/drive/MyDrive/Final_Project/input/upper_body/label_maps/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkScppydO1qG",
        "outputId": "8964fb5c-97be-4122-8b61-d89e6a7dd5d3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_extractor.py --dataset 'atr' --model-restore '/content/drive/MyDrive/Final_Project/preprocess/Self-Correction-Human-Parsing/checkpoints/final.pth' --input-dir '/content/drive/MyDrive/Final_Project/input/lower_body/images/' --output-dir '/content/drive/MyDrive/Final_Project/input/lower_body/label_maps/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv97hp1_PuPb",
        "outputId": "7b774969-af34-45af-f741-52c6285a95e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "100% 2/2 [00:00<00:00,  2.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_extractor.py --dataset 'atr' --model-restore '/content/drive/MyDrive/Final_Project/preprocess/Self-Correction-Human-Parsing/checkpoints/final.pth' --input-dir '/content/drive/MyDrive/Final_Project/input/dresses/images/' --output-dir '/content/drive/MyDrive/Final_Project/input/dresses/label_maps/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7B_6GsfQDTK",
        "outputId": "687b2c9c-3967-4bf8-c099-8cd2faff62c4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "0it [00:00, ?it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ3GLQgAQGXD",
        "outputId": "86e06bbd-c383-45d3-9655-225ebe6e7409"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJGtLNP_QQT2",
        "outputId": "ab2d64c7-6fb3-4c35-bcba-ed004e23212f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project/preprocess\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "# Note: This is a faster way to install detectron2 in Colab, but it does not include all functionalities (e.g. compiled operators).\n",
        "# See https://detectron2.readthedocs.io/tutorials/install.html for full installation instructions\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))\n",
        "\n",
        "# Properly install detectron2. (Please do not install twice in both ways)\n",
        "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKaRwZm6Qef9",
        "outputId": "7855020c-d578-4926-cd67-8d6dca5ede2d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/274.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1-cp310-cp310-linux_x86_64.whl size=44089 sha256=7988af3c3aa1c5d16e43c05502a2ca2102c8ab5f17dcaf9d2b0152fbb04237a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/83/31/975b737609aba39a4099d471d5684141c1fdc3404f97e7f68a\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: pyyaml\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask 2023.8.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\n",
            "distributed 2023.8.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1 which is incompatible.\n",
            "flax 0.7.2 requires PyYAML>=5.4.1, but you have pyyaml 5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyyaml-5.1\n",
            "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (5.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.41.2)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (3.10.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=15c17be32dada185cf8e860ca41a2b4bb4d0f9b2f25e2bfc9d703476a04de07b\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=2ae4565b181cebe9fc4c532eb805652e71f33cfe50b2967788b61e409828392a\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-23.7.0 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.11.2 portalocker-2.7.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Detectron and Cuda Avaibility"
      ],
      "metadata": {
        "id": "OAIXy78rQlCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzGbdiWWQqOW",
        "outputId": "0c82fb53-f1e7-4595-e134-5ebb58aa7cda"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n",
            "torch:  2.0 ; cuda:  cu118\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Detectorn2 for trainning"
      ],
      "metadata": {
        "id": "y07jycmSQyYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "metadata": {
        "id": "qgeBGMbdQ91o"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd detectron2/projects/DensePose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF_SioSERLOd",
        "outputId": "30fb4b57-256c-4176-c9af-714665ad585f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project/preprocess/detectron2/projects/DensePose\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting libraries with av"
      ],
      "metadata": {
        "id": "ShlzLMfpRY2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj2HDcGyRMo8",
        "outputId": "ddf8396e-e303-497e-bc62-09eff5577f41"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting av\n",
            "  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-10.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running apply_net.py with arguments"
      ],
      "metadata": {
        "id": "KdRplOE5R7uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n",
        "https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n",
        "/content/drive/MyDrive/Final_Project/input/upper_body/images dp_segm -v --output /content/drive/MyDrive/Final_Project/input/upper_body/dense/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr8JK71nSBt6",
        "outputId": "2b7b6f7c-e35d-40fb-95c5-7fa80875fbe4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[08/30 20:45:18 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_50_FPN_s1x.yaml\n",
            "\u001b[32m[08/30 20:45:19 apply_net]: \u001b[0mLoading model from https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
            "model_final_162be9.pkl: 256MB [00:01, 196MB/s]               \n",
            "\u001b[32m[08/30 20:45:26 apply_net]: \u001b[0mLoading data from /content/drive/MyDrive/Final_Project/input/upper_body/images\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/30 20:45:26 apply_net]: \u001b[0mNo input images for /content/drive/MyDrive/Final_Project/input/upper_body/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specifies the new paths for the model checkpoint, input images, and output directory"
      ],
      "metadata": {
        "id": "-kjdLglLSPUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n",
        "https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n",
        "/content/drive/MyDrive/Final_Project/input/lower_body/images dp_segm -v  --output /content/drive/MyDrive/Final_Project/input/lower_body/dense/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y00SeakZSQWp",
        "outputId": "3ffde999-9062-462c-a8d8-dd42ccf110ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[08/30 20:45:30 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_50_FPN_s1x.yaml\n",
            "\u001b[32m[08/30 20:45:30 apply_net]: \u001b[0mLoading model from https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
            "\u001b[32m[08/30 20:45:31 apply_net]: \u001b[0mLoading data from /content/drive/MyDrive/Final_Project/input/lower_body/images\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[08/30 20:45:33 apply_net]: \u001b[0mProcessing /content/drive/MyDrive/Final_Project/input/lower_body/images/1yy_0.jpg\n",
            "\u001b[32m[08/30 20:45:33 apply_net]: \u001b[0mOutput saved to /content/drive/MyDrive/Final_Project/input/lower_body/dense/1yy_5.png\n",
            "\u001b[32m[08/30 20:45:33 apply_net]: \u001b[0mProcessing /content/drive/MyDrive/Final_Project/input/lower_body/images/1San_0.jpg\n",
            "\u001b[32m[08/30 20:45:33 apply_net]: \u001b[0mOutput saved to /content/drive/MyDrive/Final_Project/input/lower_body/dense/1San_5.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apply_net.py show configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n",
        "https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n",
        "/content/drive/MyDrive/Final_Project/input/dresses/images dp_segm -v   --output /content/drive/MyDrive/Final_Project/input/dresses/dense/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twCP5XGdS3_p",
        "outputId": "2ebd88bc-b43d-422f-e6bb-16b4455fb4f7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[08/30 20:45:44 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_50_FPN_s1x.yaml\n",
            "\u001b[32m[08/30 20:45:44 apply_net]: \u001b[0mLoading model from https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
            "\u001b[32m[08/30 20:45:46 apply_net]: \u001b[0mLoading data from /content/drive/MyDrive/Final_Project/input/dresses/images\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/30 20:45:46 apply_net]: \u001b[0mNo input images for /content/drive/MyDrive/Final_Project/input/dresses/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apply_net.py dump configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n",
        "https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n",
        "/content/drive/MyDrive/Final_Project/input/upper_body/images -v --output /content/drive/MyDrive/Final_Project/input/upper_body/dense/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYfFyTEES9WN",
        "outputId": "c806e28c-1578-4151-c244-cbdffffd3308"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[08/30 20:45:50 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_50_FPN_s1x.yaml\n",
            "\u001b[32m[08/30 20:45:50 apply_net]: \u001b[0mLoading model from https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
            "\u001b[32m[08/30 20:45:52 apply_net]: \u001b[0mLoading data from /content/drive/MyDrive/Final_Project/input/upper_body/images\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/30 20:45:52 apply_net]: \u001b[0mNo input images for /content/drive/MyDrive/Final_Project/input/upper_body/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apply_net.py dump configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n",
        "https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n",
        "/content/drive/MyDrive/Final_Project/input/lower_body/images -v --output /content/drive/MyDrive/Final_Project/input/lower_body/dense/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1hx3V5OTUQd",
        "outputId": "dc201848-00a0-477e-a6d8-96122bd10268"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[08/30 20:45:56 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_50_FPN_s1x.yaml\n",
            "\u001b[32m[08/30 20:45:56 apply_net]: \u001b[0mLoading model from https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
            "\u001b[32m[08/30 20:45:59 apply_net]: \u001b[0mLoading data from /content/drive/MyDrive/Final_Project/input/lower_body/images\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "\u001b[32m[08/30 20:46:00 apply_net]: \u001b[0mProcessing /content/drive/MyDrive/Final_Project/input/lower_body/images/1yy_0.jpg\n",
            "\u001b[32m[08/30 20:46:00 apply_net]: \u001b[0mProcessing /content/drive/MyDrive/Final_Project/input/lower_body/images/1San_0.jpg\n",
            "\u001b[32m[08/30 20:46:00 apply_net]: \u001b[0mOutput saved to /content/drive/MyDrive/Final_Project/input/lower_body/dense/1yy_5_uv.npz\n",
            "\u001b[32m[08/30 20:46:00 apply_net]: \u001b[0mOutput saved to /content/drive/MyDrive/Final_Project/input/lower_body/dense/1San_5_uv.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python apply_net.py dump configs/densepose_rcnn_R_50_FPN_s1x.yaml \\\n",
        "https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl \\\n",
        "/content/drive/MyDrive/Final_Project/input/dresses/images -v --output /content/drive/MyDrive/Final_Project/input/dresses/dense/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf188TOI0PEx",
        "outputId": "8a7a8941-649f-41c8-f0e6-e0e303faab49"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[08/30 20:46:04 apply_net]: \u001b[0mLoading config from configs/densepose_rcnn_R_50_FPN_s1x.yaml\n",
            "\u001b[32m[08/30 20:46:04 apply_net]: \u001b[0mLoading model from https://dl.fbaipublicfiles.com/densepose/densepose_rcnn_R_50_FPN_s1x/165712039/model_final_162be9.pkl\n",
            "\u001b[32m[08/30 20:46:06 apply_net]: \u001b[0mLoading data from /content/drive/MyDrive/Final_Project/input/dresses/images\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/30 20:46:06 apply_net]: \u001b[0mNo input images for /content/drive/MyDrive/Final_Project/input/dresses/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Folder"
      ],
      "metadata": {
        "id": "hsDpYruDTm4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPIqBTU1T_Sf",
        "outputId": "3c90a526-7c52-4c6e-cf3f-172345c22902"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process grayscale images under specific directories within the input directory."
      ],
      "metadata": {
        "id": "tuK5DzbOUg9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import glob\n",
        "pattern = 'input/*/dense/*'\n",
        "mp ={0: 0, 128: 18, 64: 4, 132: 19, 69: 5, 136: 20, 75: 6, 140: 21, 145: 22, 85: 9, 150: 23, 90: 10, 155: 24, 121: 16, 105: 13, 111: 14, 52: 2, 117: 15, 57: 3, 124: 17,\n",
        "     2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 9: 9, 10: 10, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 24: 24}\n",
        "\n",
        "lut = np.zeros((256, 1), dtype=np.uint8)\n",
        "\n",
        "for i in range(0,256):\n",
        "    lut[i] = mp.get(i) or mp[min(mp.keys(), key = lambda key: abs(key-i))]\n",
        "\n",
        "for images in glob.glob(pattern):\n",
        "    if images.endswith(\".png\"):\n",
        "      image = cv2.imread(images,cv2.IMREAD_GRAYSCALE)\n",
        "      cv2.imwrite(images,cv2.LUT(image,lut))"
      ],
      "metadata": {
        "id": "kLUBRhD0Uife"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dictionary provides a mapping for a range of original pixel values to new pixel values. For example, if an original pixel value in the image is 128, it will be remapped to 18"
      ],
      "metadata": {
        "id": "iN_YzF9RU4bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code scans through files in subdirectories under the input directory. If a file's name ends with _1.jpg or _1.png, the code removes the file using the os.remove() function."
      ],
      "metadata": {
        "id": "ME-4zrWeVLPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Clear\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "files = glob.glob('input/*/*/*.*')\n",
        "for f in files:\n",
        "  if f.endswith(\"_1.jpg\") or f.endswith(\"_1.png\"):\n",
        "    os.remove(f)"
      ],
      "metadata": {
        "id": "rw79sA4dU53J"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resizing and Padding is added"
      ],
      "metadata": {
        "id": "mIU-m18yVURh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_with_pad(im, target_width, target_height):\n",
        "    '''\n",
        "    Resize PIL image keeping ratio and using white background.\n",
        "    '''\n",
        "    target_ratio = target_height / target_width\n",
        "    im_ratio = im.height / im.width\n",
        "    if target_ratio > im_ratio:\n",
        "        # It must be fixed by width\n",
        "        resize_width = target_width\n",
        "        resize_height = round(resize_width * im_ratio)\n",
        "    else:\n",
        "        # Fixed by height\n",
        "        resize_height = target_height\n",
        "        resize_width = round(resize_height / im_ratio)\n",
        "\n",
        "    image_resize = im.resize((resize_width, resize_height), Image.ANTIALIAS)\n",
        "    background = Image.new('RGBA', (target_width, target_height), (255, 255, 255, 255))\n",
        "    offset = (round((target_width - resize_width) / 2), round((target_height - resize_height) / 2))\n",
        "    background.paste(image_resize, offset)\n",
        "    return background.convert('RGB')"
      ],
      "metadata": {
        "id": "dsy2fPmXVUwZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves the processed images back to the same paths.\n"
      ],
      "metadata": {
        "id": "45qDW_9UVpFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in ['dresses','upper_body','lower_body']:\n",
        "  files = glob.glob('images/'+c+'/*.*')\n",
        "  path = 'input/' + c + '/images/'\n",
        "  for f in files:\n",
        "    if f.endswith(\"_1.jpg\"):\n",
        "      res = path +os.path.basename(f)\n",
        "      shutil.copy (f, res)\n",
        "      image = Image.open(res)\n",
        "      new = resize_with_pad(image,384,512)\n",
        "      new.save(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67hA5zqYVpzz",
        "outputId": "4333fa84-dac1-4f3b-93d7-396ab346b4cf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-e04c8aed8ec9>:16: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image_resize = im.resize((resize_width, resize_height), Image.ANTIALIAS)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processes images to match clothing category"
      ],
      "metadata": {
        "id": "wsSaB3UJV_kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import copy\n",
        "import glob\n",
        "import json\n",
        "import os\n",
        "\n",
        "def otsu(img , n  , x ):\n",
        "    img_gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.adaptiveThreshold(img_gray,255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,n,x)\n",
        "    return thresh\n",
        "\n",
        "def contour(img):\n",
        "    edges = cv2.dilate(cv2.Canny(img,200,255),None)\n",
        "    cnt = sorted(cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[-2], key=cv2.contourArea)[-1]\n",
        "    mask = np.zeros((img.shape[0],img.shape[1]), np.uint8)\n",
        "    masked = cv2.drawContours(mask, [cnt],-1, 255, -1)\n",
        "    return masked\n",
        "\n",
        "def get_cloth_mask(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    mask = np.zeros_like(image)\n",
        "    cv2.drawContours(mask, contours, -1, (255, 255, 255), -1)\n",
        "    return mask\n",
        "\n",
        "def write_edge(C_path,E_path):\n",
        "    img = cv2.imread(C_path)\n",
        "    res = get_cloth_mask(img)\n",
        "    if(np.mean(res)<100):\n",
        "        ot = otsu(img,11,0.6)\n",
        "        res = contour(ot)\n",
        "    cv2.imwrite(E_path,res)\n",
        "\n",
        "for s in ['upper_body','lower_body','dresses']:\n",
        "  input_path = '/content/drive/MyDrive/Final_Project/input/' + s + '/images/'\n",
        "  output_path = '/content/drive/MyDrive/Final_Project/input/'+ s + '/masks/'\n",
        "  for images in glob.glob('*',root_dir = input_path):\n",
        "      if images.endswith(\"_1.jpg\"):\n",
        "        write_edge(input_path + images , output_path+ os.path.splitext(images)[0] +\".png\")"
      ],
      "metadata": {
        "id": "K1jHc6RhWAOW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning Up the objects that are no longer needed"
      ],
      "metadata": {
        "id": "UGtyFyWlWfWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtOo-vElWf35",
        "outputId": "9a20601a-e733-42cd-bb20-6beda8db21d6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test.sh contains command for model to activate for **Cloth Swap**"
      ],
      "metadata": {
        "id": "_gQenywfWzRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sh test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pCTYf9uXAsX",
        "outputId": "559dcc3c-c825-457f-9c7d-a8a341afc245"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-30 20:51:04.234292: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.embeddings.token_embedding.weight', 'logit_scale', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /content/drive/MyDrive/ladi-vton/checkpoints/miccunifi_ladi-vton_master\n",
            "Using cache found in /content/drive/MyDrive/ladi-vton/checkpoints/miccunifi_ladi-vton_master\n",
            "Using cache found in /content/drive/MyDrive/ladi-vton/checkpoints/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /content/drive/MyDrive/ladi-vton/checkpoints/miccunifi_ladi-vton_master\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Fin\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33mal_Project/src/\u001b[0m\u001b[1;33minference.py\u001b[0m:\u001b[94m321\u001b[0m in \u001b[92m<module>\u001b[0m                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m318 \u001b[0m\u001b[2m│   \u001b[0mtorch.cuda.empty_cache()                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m319 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m320 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m321 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m322 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mdecorate_context\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Fin\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33mal_Project/src/\u001b[0m\u001b[1;33minference.py\u001b[0m:\u001b[94m211\u001b[0m in \u001b[92mmain\u001b[0m                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m208 \u001b[0m\u001b[2m│   \u001b[0mgenerator = torch.Generator(\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m).manual_seed(args.seed)         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m209 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m210 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Generate the images\u001b[0m                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m211 \u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m idx, batch \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(tqdm(test_dataloader)):                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m212 \u001b[0m\u001b[2m│   │   \u001b[0mmodel_img = batch.get(\u001b[33m\"\u001b[0m\u001b[33mimage\u001b[0m\u001b[33m\"\u001b[0m).to(weight_dtype)                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m213 \u001b[0m\u001b[2m│   │   \u001b[0mmask_img = batch.get(\u001b[33m\"\u001b[0m\u001b[33minpaint_mask\u001b[0m\u001b[33m\"\u001b[0m).to(weight_dtype)          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m214 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m mask_img \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/tqdm/\u001b[0m\u001b[1;33mstd.py\u001b[0m:\u001b[94m1182\u001b[0m in \u001b[92m__iter__\u001b[0m         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1179 \u001b[0m\u001b[2m│   │   \u001b[0mtime = \u001b[96mself\u001b[0m._time                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1180 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1181 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1182 \u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m obj \u001b[95min\u001b[0m iterable:                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1183 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94myield\u001b[0m obj                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1184 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Update and possibly print the progressbar.\u001b[0m          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1185 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# Note: does not call self.update(1) for speed optimi\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/accelerate/\u001b[0m\u001b[1;33mdata_loader.py\u001b[0m:\u001b[94m378\u001b[0m in     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m__iter__\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m375 \u001b[0m\u001b[2m│   │   \u001b[0mdataloader_iter = \u001b[96msuper\u001b[0m().\u001b[92m__iter__\u001b[0m()                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m376 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# We iterate one batch ahead to check when we are at the end\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m377 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m378 \u001b[2m│   │   │   \u001b[0mcurrent_batch = \u001b[96mnext\u001b[0m(dataloader_iter)                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m379 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mStopIteration\u001b[0m:                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m380 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94myield\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m381 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m633\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m__next__\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 630 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._sampler_iter \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 631 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[2m# TODO(https://github.com/pytorch/pytorch/issues/7675\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 632 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._reset()  \u001b[2m# type: ignore[call-arg]\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 633 \u001b[2m│   │   │   \u001b[0mdata = \u001b[96mself\u001b[0m._next_data()                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 634 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._num_yielded += \u001b[94m1\u001b[0m                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 635 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._dataset_kind == _DatasetKind.Iterable \u001b[95mand\u001b[0m \\      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 636 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._IterableDataset_len_called \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m1345\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_next_data\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1342 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m._task_info[idx] += (data,)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1343 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1344 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mdel\u001b[0m \u001b[96mself\u001b[0m._task_info[idx]                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1345 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._process_data(data)                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1346 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1347 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_try_put_index\u001b[0m(\u001b[96mself\u001b[0m):                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1348 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mself\u001b[0m._tasks_outstanding < \u001b[96mself\u001b[0m._prefetch_factor * \u001b[96mself\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/utils/data/\u001b[0m\u001b[1;33mdataloader.py\u001b[0m:\u001b[94m1371\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92m_process_data\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1368 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._rcvd_idx += \u001b[94m1\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1369 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m._try_put_index()                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1370 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(data, ExceptionWrapper):                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1371 \u001b[2m│   │   │   \u001b[0mdata.reraise()                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1372 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m data                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1373 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1374 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_mark_worker_as_unavailable\u001b[0m(\u001b[96mself\u001b[0m, worker_id, shutdown=\u001b[94mFalse\u001b[0m): \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m644\u001b[0m in \u001b[92mreraise\u001b[0m       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m641 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# If the exception takes multiple arguments, don't try to\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m642 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# instantiate since we don't know how to\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(msg) \u001b[94mfrom\u001b[0m \u001b[94mNone\u001b[0m                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m644 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m exception                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m645 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m646 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m647 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_available_device_type\u001b[0m():                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mFileNotFoundError: \u001b[0mCaught FileNotFoundError in DataLoader worker process \u001b[1;36m0\u001b[0m.\n",
            "Original Traceback \u001b[1m(\u001b[0mmost recent call last\u001b[1m)\u001b[0m:\n",
            "  File \n",
            "\u001b[32m\"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\"\u001b[0m, \n",
            "line \u001b[1;36m308\u001b[0m, in _worker_loop\n",
            "    data = \u001b[1;35mfetcher.fetch\u001b[0m\u001b[1m(\u001b[0mindex\u001b[1m)\u001b[0m\n",
            "  File \n",
            "\u001b[32m\"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\"\u001b[0m, line\n",
            "\u001b[1;36m51\u001b[0m, in fetch\n",
            "    data = \u001b[1m[\u001b[0mself.dataset\u001b[1m[\u001b[0midx\u001b[1m]\u001b[0m for idx in possibly_batched_index\u001b[1m]\u001b[0m\n",
            "  File \n",
            "\u001b[32m\"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\"\u001b[0m, line\n",
            "\u001b[1;36m51\u001b[0m, in \u001b[1m<\u001b[0m\u001b[1;95mlistcomp\u001b[0m\u001b[1m>\u001b[0m\n",
            "    data = \u001b[1m[\u001b[0mself.dataset\u001b[1m[\u001b[0midx\u001b[1m]\u001b[0m for idx in possibly_batched_index\u001b[1m]\u001b[0m\n",
            "  File \n",
            "\u001b[32m\"/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_\u001b[0m\n",
            "\u001b[32mProject/src/dataset/dresscode.py\"\u001b[0m, line \u001b[1;36m125\u001b[0m, in __getitem__\n",
            "    cloth = \u001b[1;35mImage.open\u001b[0m\u001b[1m(\u001b[0m\u001b[1;35mos.path.join\u001b[0m\u001b[1m(\u001b[0mdataroot, \u001b[32m'images'\u001b[0m, c_name\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
            "  File \u001b[32m\"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\"\u001b[0m, line \u001b[1;36m3227\u001b[0m, in \n",
            "open\n",
            "    fp = \u001b[1;35mbuiltins.open\u001b[0m\u001b[1m(\u001b[0mfilename, \u001b[32m\"rb\"\u001b[0m\u001b[1m)\u001b[0m\n",
            "FileNotFoundError: \u001b[1m[\u001b[0mErrno \u001b[1;36m2\u001b[0m\u001b[1m]\u001b[0m No such file or directory: \n",
            "\u001b[32m'./input/lower_body/images/shirt_1.jpg'\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!sh metrics.sh"
      ],
      "metadata": {
        "id": "8qTR4YHoXHcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show Output"
      ],
      "metadata": {
        "id": "_GxKCG5JXUm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import glob\n",
        "\n",
        "pattern = 'results/unpaired/*/*'\n",
        "for images in glob.glob(pattern):\n",
        "    if (images.endswith(\".png\") or images.endswith(\".jpg\") or images.endswith(\".jpeg\")):\n",
        "      cv2_imshow(cv2.imread(images, cv2.IMREAD_UNCHANGED))"
      ],
      "metadata": {
        "id": "iUhorVqBXfYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Face correction"
      ],
      "metadata": {
        "id": "li14fZUHXtM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Final_Project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy8iFmJMX9Kg",
        "outputId": "42f52c50-f5b5-446b-c2f4-8693fc044a8b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Uo1iJ5_7Fa_nSfXnCVNDUqF80C17LcjE/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "-z_nAGXzdTsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e256203-09aa-4352-d9fa-58aef5192987"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.3 sounddevice-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "import mediapipe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "1lMdWVnTdiPT"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dresscode = 'final'"
      ],
      "metadata": {
        "id": "XNfSup7idjdq"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filepath = os.path.join('input', f\"test_pairs_paired.txt\")\n",
        "# with open(filepath, 'r') as f:\n",
        "#     lines = f.read().splitlines()\n",
        "# org_paths = sorted(\n",
        "#       [os.path.join('input', category, 'images', line.strip().split()[0]) for line in lines for\n",
        "#         category in ['lower_body', 'upper_body', 'dresses'] if\n",
        "#         os.path.exists(os.path.join('input', category, 'images', line.strip().split()[0]))])\n",
        "# res_paths = sorted(\n",
        "#                 [os.path.join('results/unpaired', category, name) for category in ['lower_body', 'upper_body', 'dresses'] for\n",
        "#                  name in os.listdir(os.path.join('results/unpaired', category)) if\n",
        "#                  os.path.exists(os.path.join('results/unpaired', category, name))])"
      ],
      "metadata": {
        "id": "L7j7sygHdomk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Number of org_paths:\", len(org_paths))\n",
        "# print(\"Number of res_paths:\", len(res_paths))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pzc6XPtBZe9",
        "outputId": "08b7ddd4-6660-4603-9b69-f6c979973f76"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of org_paths: 3\n",
            "Number of res_paths: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# assert len(org_paths) == len(res_paths)\n",
        "sz = len(org_paths)"
      ],
      "metadata": {
        "id": "j91Vcquyd_2u"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for iter in range(0,sz):\n",
        "#   org_img = cv2.imread(org_paths[iter])\n",
        "#   org_res = cv2.imread(res_paths[iter])\n",
        "#   h,w = int(org_img.shape[0]/2),org_img.shape[1]\n",
        "#   img = org_img[:h,:w]\n",
        "#   res = org_res[:h,:w]\n",
        "#   cv2_imshow(res)\n",
        "#   mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "#   face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "#   results = face_mesh.process(img[:,:,::-1])\n",
        "#   if(results.multi_face_landmarks == None):\n",
        "#      print('miss')\n",
        "#      continue\n",
        "#   landmarks = results.multi_face_landmarks[0]\n",
        "#   df = pd.DataFrame(list(mp_face_mesh.FACEMESH_FACE_OVAL),columns=['p1','p2'])\n",
        "#   routes_idx = []\n",
        "\n",
        "#   p2 = df.iloc[0]['p1']\n",
        "#   p2 = df.iloc[0]['p2']\n",
        "#   for i in range(0,df.shape[0]):\n",
        "#     obj = df[df['p1'] == p2]\n",
        "#     p1 = obj['p1'].values[0]\n",
        "#     p2 = obj['p2'].values[0]\n",
        "\n",
        "#     cur = []\n",
        "#     cur.append(p1)\n",
        "#     cur.append(p2)\n",
        "#     routes_idx.append(cur)\n",
        "\n",
        "#   routes = []\n",
        "#   for sid,tid in routes_idx:\n",
        "#     sxy = landmarks.landmark[sid]\n",
        "#     txy = landmarks.landmark[tid]\n",
        "\n",
        "#     source = (int(sxy.x * img.shape[1]) , int(sxy.y * img.shape[0]))\n",
        "#     target = (int(txy.x * img.shape[1]) , int(txy.y * img.shape[0]))\n",
        "\n",
        "#     routes.append(source)\n",
        "#     routes.append(target)\n",
        "\n",
        "#   mask = np.zeros((img.shape[0],img.shape[1]))\n",
        "#   mask = cv2.fillConvexPoly(mask,np.array(routes),1)\n",
        "#   mask = mask.astype(bool)\n",
        "#   res[mask] = img[mask]\n",
        "#   cv2_imshow(res)\n",
        "#   org_img[:h,:w] = img\n",
        "#   org_res[:h,:w] = res\n",
        "#   cv2.imwrite(res_paths[iter].replace('results/unpaired','final').replace('_0.jpg','_'+dresscode+'.jpg'),org_res)"
      ],
      "metadata": {
        "id": "j7t9t3PceBQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dresscode = 'final'\n",
        "filepath = os.path.join('input', f\"test_pairs_paired.txt\")\n",
        "with open(filepath, 'r') as f:\n",
        "    lines = f.read().splitlines()\n",
        "org_paths = sorted(\n",
        "      [os.path.join('input', category, 'images', line.strip().split()[0]) for line in lines for\n",
        "        category in ['lower_body', 'upper_body', 'dresses'] if\n",
        "        os.path.exists(os.path.join('input', category, 'images', line.strip().split()[0]))])\n",
        "res_paths = sorted(\n",
        "                [os.path.join('results/unpaired', category, name) for category in ['lower_body', 'upper_body', 'dresses'] for\n",
        "                 name in os.listdir(os.path.join('results/unpaired', category)) if\n",
        "                 os.path.exists(os.path.join('results/unpaired', category, name))])\n",
        "assert len(org_paths) == len(res_paths)\n",
        "sz = len(org_paths)\n",
        "\n",
        "for iter in range(0, sz):\n",
        "    org_img = cv2.imread(org_paths[iter])\n",
        "    org_res = cv2.imread(res_paths[iter])\n",
        "\n",
        "    cv2_imshow(org_res)\n",
        "\n",
        "    mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "    results = face_mesh.process(org_img[:,:,::-1])\n",
        "\n",
        "    if results.multi_face_landmarks is None:\n",
        "        print('Miss: No face landmarks found.')\n",
        "        continue\n",
        "\n",
        "    landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "    df = pd.DataFrame(list(mp_face_mesh.FACEMESH_FACE_OVAL), columns=['p1', 'p2'])\n",
        "    routes_idx = []\n",
        "\n",
        "    p2 = df.iloc[0]['p1']\n",
        "    p2 = df.iloc[0]['p2']\n",
        "\n",
        "    for i in range(0, df.shape[0]):\n",
        "        obj = df[df['p1'] == p2]\n",
        "        p1 = obj['p1'].values[0]\n",
        "        p2 = obj['p2'].values[0]\n",
        "\n",
        "        cur = []\n",
        "        cur.append(p1)\n",
        "        cur.append(p2)\n",
        "        routes_idx.append(cur)\n",
        "\n",
        "    routes = []\n",
        "\n",
        "    for sid, tid in routes_idx:\n",
        "        sxy = landmarks.landmark[sid]\n",
        "        txy = landmarks.landmark[tid]\n",
        "\n",
        "        source = (int(sxy.x * org_img.shape[1]), int(sxy.y * org_img.shape[0]))\n",
        "        target = (int(txy.x * org_img.shape[1]), int(txy.y * org_img.shape[0]))\n",
        "\n",
        "        routes.append(source)\n",
        "        routes.append(target)\n",
        "\n",
        "    mask = np.zeros((org_img.shape[0], org_img.shape[1]))\n",
        "    mask = cv2.fillConvexPoly(mask, np.array(routes), 1)\n",
        "    mask = mask.astype(bool)\n",
        "    org_res[mask] = org_img[mask]\n",
        "\n",
        "    cv2_imshow(org_res)\n",
        "\n",
        "    cv2.imwrite(res_paths[iter].replace('results/unpaired', 'final').replace('_0.jpg', '_' + dresscode + '.jpg'), org_res)\n"
      ],
      "metadata": {
        "id": "AsFkrCEJD3DF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}